
# source_type in [gcs, pubsub, ...]
# TODO: might make this dynamically determined

# input can be a:
    # local path (e.g. ./test.txt)
    # GCS path (e.g. ...),
    # pubsub subscription (e.g. ...)
#inputPath = ./data/raw_data.json
# outputPath = test_output
#input_data.type = txt

# inputType/outputType in [gcs, pubsub, ...]
inputType = pubsub
inputGcsPath = gs://test-bucket-85203/dataflow_test/raw_data.json
inputPubsubSubscription = test-input-sub
outputType = pubsub
outputGcsPath = gs://test-bucket-85203/dataflow_test/test_output
outputPubsubTopic = test-output


### Side Inputs
# custInfoSideInputPath = ./data/cust_info.json
# profileSideInputPath = ./data/profile.json


modelPath = ./src/main/resources/model.bin
# modelPath =  gs://test-bucket-85203/dataflow_test/model.bin

# TODO(dev): add service account key to file (or authenticate a different way)
serviceAccountKeyPath = ./service_account_key.json

# Dataflow config
project = analog-arbor-367702
bucket = test-bucket-85203
custInfoSideInputPrefix = dataflow_test/cust_info.json
profileSideInputPrefix = dataflow_test/profile.json
numWorkers = 1
maxNumWorkers = 5
serviceAccountEmail = service-account1@analog-arbor-367702.iam.gserviceaccount.com
machineType = n1-standard-2
region = us-central1
isStreaming = true
sdkContainerImage = gcr.io/analog-arbor-367702/model-serving:latest
runner = dataflow
