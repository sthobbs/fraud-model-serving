

# source_type in [disk, pubsub, ...]
# TODO: might make this dynamically determined
inputType = disk

# input can be a:
    # local path (e.g. ./test.txt)
    # GCS path (e.g. ...),
    # pubsub subscription (e.g. ...)
#inputPath = ./data/raw_data.json
inputPath = gs://test-bucket-85203/dataflow_test/raw_data.json
#input_data.type = txt

# input_data_source_type in [disk, pubsub, ...]
outputType = disk
# outputPath = test_output
outputPath = gs://test-bucket-85203/dataflow_test/test_output

### Side Inputs
# custInfoPath = ./data/cust_info.json
custInfoPath = gs://test-bucket-85203/dataflow_test/cust_info.json
# profilePath = ./data/profile.json
profilePath = gs://test-bucket-85203/dataflow_test/profile.json


modelPath = ./src/main/resources/model.bin
# modelPath =  gs://test-bucket-85203/dataflow_test/model.bin

# TODO(dev): add service account key to file
serviceAccountKeyPath = ./service_account_key.json

# Dataflow config
projectId = analog-arbor-367702
numWorkers = 1
maxNumWorkers = 5
serviceAccountEmail = service-account1@analog-arbor-367702.iam.gserviceaccount.com
machineType = n1-standard-2
region = us-central1
isStreaming = true



